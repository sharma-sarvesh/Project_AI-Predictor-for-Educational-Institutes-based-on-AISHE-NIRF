{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d1bf28b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59ffd53d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08d1e28a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "data = pd.read_csv(\"EngineeringRanking_Final.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "510173cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handling missing values by filling with 0\n",
    "data.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e8cad98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove rows with missing target variable (Rank_21)\n",
    "data.dropna(subset=['Rank_21'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f341147",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature selection\n",
    "features = ['Score_21', 'Rank_21', 'TLR_21', 'RPC_21', 'GO_21', 'OI_21', 'Perception_21']\n",
    "X = data[features]\n",
    "y = data['Rank_21']  # Target variable for prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bf958c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting data into train, test, and validation sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61645623",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Train the model\n",
    "model = XGBRegressor()\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8785cc3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predictions for 2023 data\n",
    "data.dropna(subset=['Rank_21'], inplace=True)  # Remove rows with missing target values\n",
    "X_val = data[features]\n",
    "y_val_true = data['Rank_21']\n",
    "y_val_pred = model.predict(X_val)\n",
    "data['Predicted_Rank_2023'] = y_val_pred\n",
    "sorted_data_2023 = data.sort_values(by='Predicted_Rank_2023')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48e23a36",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Displaying the predicted Rank_23 for colleges\n",
    "Predicted_Rank_23 = sorted_data_2023[['Institute Name', 'City', 'State', 'Rank_21', 'Predicted_Rank_2023']]\n",
    "print(\"\\nPredicted Rank for 2023:\")\n",
    "print(Predicted_Rank_23)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e19ee09",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Accuracy:\n",
    "\n",
    "# Calculate accuracy\n",
    "threshold = 1  # Define the threshold for accurate predictions (e.g., within Â±1 rank)\n",
    "\n",
    "# Count the number of accurate predictions\n",
    "accurate_predictions = np.sum(np.abs(y_val_true - y_val_pred) <= threshold)\n",
    "\n",
    "# Total number of predictions\n",
    "total_predictions = len(y_val_true)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = (accurate_predictions / total_predictions) * 100\n",
    "print(f\"Accuracy on Validation Set (2023 data): {accuracy:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81f82bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on training set\n",
    "y_train_pred = model.predict(X_train)\n",
    "train_mse = mean_squared_error(y_train, y_train_pred)\n",
    "train_mae = mean_absolute_error(y_train, y_train_pred)\n",
    "train_r2 = r2_score(y_train, y_train_pred)\n",
    "print(\"Training Set Evaluation:\")\n",
    "print(f\"Mean Squared Error on Training Set: {train_mse}\")\n",
    "print(f\"Mean Absolute Error on Training Set: {train_mae}\")\n",
    "print(f\"R-squared on Training Set: {train_r2}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adf2d73e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on validation set\n",
    "y_val_pred = model.predict(X_val)\n",
    "val_mse = mean_squared_error(y_val_true, y_val_pred)\n",
    "val_mae = mean_absolute_error(y_val_true, y_val_pred)\n",
    "val_r2 = r2_score(y_val_true, y_val_pred)\n",
    "print(\"\\nValidation Set Evaluation:\")\n",
    "print(f\"Mean Squared Error on Validation Set: {val_mse}\")\n",
    "print(f\"Mean Absolute Error on Validation Set: {val_mae}\")\n",
    "print(f\"R-squared on Validation Set: {val_r2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fffa9c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on testing set\n",
    "y_test_pred = model.predict(X_test)\n",
    "test_mse = mean_squared_error(y_test, y_test_pred)\n",
    "test_mae = mean_absolute_error(y_test, y_test_pred)\n",
    "test_r2 = r2_score(y_test, y_test_pred)\n",
    "print(\"\\nTesting Set Evaluation:\")\n",
    "print(f\"Mean Squared Error on Testing Set: {test_mse}\")\n",
    "print(f\"Mean Absolute Error on Testing Set: {test_mae}\")\n",
    "print(f\"R-squared on Testing Set: {test_r2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12e0c6df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for overfitting or underfitting\n",
    "if train_r2 > val_r2:\n",
    "    print(\"\\nModel is overfitting (training R-squared > validation R-squared)\")\n",
    "elif train_r2 < val_r2:\n",
    "    print(\"\\nModel is underfitting (training R-squared < validation R-squared)\")\n",
    "else:\n",
    "    print(\"\\nModel is performing well (training R-squared == validation R-squared)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab81fa71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataFrame to compare actual and predicted ranks\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Institute Name': data['Institute Name'],\n",
    "    'City': data['City'],\n",
    "    'State': data['State'],\n",
    "    'Actual Rank 2023': y_val_true,\n",
    "    'Predicted Rank 2023': y_val_pred.round().astype(int)  # Round predicted ranks to the nearest integer\n",
    "})\n",
    "\n",
    "# Display the comparison DataFrame\n",
    "print(\"Comparison of Actual and Predicted Ranks for 2023:\")\n",
    "print(comparison_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "315a489a",
   "metadata": {},
   "source": [
    "# CLUSTERING ALGORITHM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4561c938",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58460ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"EngineeringRanking_Final.csv\")\n",
    "data.fillna(0, inplace=True)\n",
    "data.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22dd99b5",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def display_city_list(state):\n",
    "    cities = data[data['State'] == state]['City'].unique()\n",
    "    print(\"\\nCities in\", state, \"are:\")\n",
    "    for city in cities:\n",
    "        print(city)\n",
    "\n",
    "def display_state_list():\n",
    "    states = data['State'].unique()\n",
    "    print(\"States with institutes are:\")\n",
    "    for state in states:\n",
    "        print(state)\n",
    "        \n",
    "def display_institutes_by_city(city):\n",
    "    city_data = data[data['City'] == city]\n",
    "    if len(city_data) == 0:\n",
    "        print(\"No institutes found in\", city)\n",
    "        return\n",
    "    print(\"\\nInstitutes in\", city, \"from 2016 to 2021:\")\n",
    "    print('-' * 150)\n",
    "    print(\"{:<70} {:<10} {:<10} {:<10} {:<10} {:<10} {:<10}\".format(\n",
    "        \"Institute Name\", \"2016\", \"2017\", \"2018\", \"2019\", \"2020\", \"2021\"))\n",
    "    print('-' * 150)\n",
    "    for index, row in city_data.iterrows():\n",
    "        print(\"{:<70} {:<10} {:<10} {:<10} {:<10} {:<10} {:<10}\".format(\n",
    "            row['Institute Name'],\n",
    "            row['Rank_16'], row['Rank_17'], row['Rank_18'], row['Rank_19'], row['Rank_20'], row['Rank_21']\n",
    "        ))\n",
    "    print('-' * 150)\n",
    "\n",
    "\n",
    "# Main program\n",
    "display_state_list()\n",
    "state = input(\"Enter state: \")\n",
    "display_city_list(state)\n",
    "city = input(\"Enter City: \")\n",
    "display_institutes_by_city(city)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34bfa7e2",
   "metadata": {},
   "source": [
    "# Prediction of Rank_24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "351ed8e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from sklearn.ensemble import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c96c93d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data into a pandas DataFrame\n",
    "data = pd.read_csv(\"EngineeringRanking_Final.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2ceab7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace all NaN values with 0\n",
    "data.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71f51e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape the data into a time series format\n",
    "time_series_data = data.melt(id_vars=['Institute Id', 'Institute Name', 'City', 'State'],\n",
    "                             value_vars=['Rank_23', 'Score_23', 'TLR_23', 'RPC_23', 'GO_23', 'OI_23', 'Perception_23'],\n",
    "                             var_name='Metric', value_name='Value')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deab85ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group the data by institute and create a time series for each institute\n",
    "grouped = time_series_data.groupby(['Institute Id', 'Institute Name', 'City', 'State', 'Metric'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "917acc31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train an ARIMA model for each institute's time series\n",
    "forecasts = []\n",
    "for group_name, group in grouped:\n",
    "    institute_id, institute_name, city, state, metric = group_name\n",
    "    if metric == 'Rank':\n",
    "        model = ARIMA(group['Value'], order=(1, 1, 1))\n",
    "        model_fit = model.fit()\n",
    "        forecast = model_fit.forecast(steps=1)\n",
    "        forecasts.append([institute_id, institute_name, city, state, metric, forecast[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32ba7d9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternatively, train a Random Forest Regression model\n",
    "features = ['Score_23', 'TLR_23', 'RPC_23', 'GO_23', 'OI_23', 'Perception_23']\n",
    "target = 'Rank_23'\n",
    "X = data[features]\n",
    "y = data[target]\n",
    "model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "model.fit(X, y)\n",
    "predictions = model.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faafbc25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Round the predictions to the nearest whole number\n",
    "predictions_rounded = [round(pred) for pred in predictions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ae66a3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the predictions for 2024\n",
    "predictions_2024 = pd.DataFrame({'Institute Id': data['Institute Id'],\n",
    "                                  'Institute Name': data['Institute Name'],\n",
    "                                  'City': data['City'],\n",
    "                                  'State': data['State'],\n",
    "                                  'Predicted Rank 2024': predictions_rounded})\n",
    "\n",
    "print(predictions_2024)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "602c0efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save predictions_2024 into a CSV file\n",
    "predictions_2024.to_csv(\"Predictions of Rank_24.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7916f007",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22c2b950",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
